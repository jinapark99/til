# 🗓️ 2025-07-21 (MON)

> **Today I learned**  
How to calculate binomial probabilities using Pascal’s Triangle and Python, especially for modeling Galton boards. I also reviewed best practices in Python like using generator expressions with `sum()`, f-strings for formatting, and recognized how small syntax errors can lead to bugs.

---

## 📘 Discrete Math – Pascal’s Triangle & Binomial Distribution

Studied the connection between Pascal’s Triangle, combinations, and probability.  
Covered:

- **Pascal’s Triangle**: Each row contains values of \( \binom{n}{k} \)  
- **Binomial Probability**: Expressed as \( \frac{\binom{n}{k}}{2^n} \), applicable to Galton boards  
- **Range Summation**: For bins 40–60 in a 100-layer Galton board, compute \( \sum_{k=40}^{60} \binom{100}{k} \)  
- **Normal Approximation**: For large n (like 1000), binomial distribution approximates normal distribution  
- **Central Limit Theorem**: More layers → stronger central concentration in outcomes

---

## 💡 Insight

- Galton boards physically represent binomial distributions.
- As the number of layers increases, the distribution of outcomes clusters tightly around the mean.
- Summing over a **range of bins** (not just one) is essential when calculating cumulative probability.
- `sum(math.comb(...))` using a generator is both memory-efficient and Pythonic.
- f-strings dramatically improve code readability and output clarity.
- Clean and readable code isn't just about style—it's about **preventing subtle logic errors.**

---

## 🐍 Python Practice

### Problem 1 – Exact probability (bins 40–60, 100 layers)

```python
import math

total = 0
for i in range(40, 61):
    total += math.comb(100, i)

fraction = total / (2 ** 100)
print(f"Answer to Q1: {fraction:.5f}")  # Output: 0.96385
```

### Problem 2 – Approximate probability (bins 400–600, 1000 layers)

```python
from scipy.stats import norm

mu = 500
sigma = (1000 * 0.25) ** 0.5

p = norm.cdf(600, loc=mu, scale=sigma) - norm.cdf(400, loc=mu, scale=sigma)
print(f"Answer to Q2: {p:.10f}")  # Output: 0.9999999986
```

---

## 🐞 Common Mistakes & Fixes

| Mistake | Issue | Fixed |
|--------|-------|--------|
| `total =+ value` | Reassigns instead of accumulating | `total += value` |
| `for i in (40, 60):` | Only iterates twice (tuple), not a range | `for i in range(40, 61):` |
| `print(f"Answer: {total2.f5}")` | `.f5` is not valid syntax | `print(f"Answer: {total2:.5f}")` |
| `print("Answer: {value}")` | Without `f`, `{}` is not evaluated | `print(f"Answer: {value}")` |

---

## 📄 Problem Summary – Galton Board Probabilities

```python
# Q1 – 100 layers, bins 40–60
prob1 = sum(math.comb(100, k) for k in range(40, 61)) / (2 ** 100)

# Q2 – 1000 layers, bins 400–600 (approximate using normal dist)
from scipy.stats import norm
prob2 = norm.cdf(600, loc=500, scale=(1000 * 0.25)**0.5) - norm.cdf(400, loc=500, scale=(1000 * 0.25)**0.5)
```

---

## 🧠 Concepts Practiced

- ✅ Pascal’s Triangle & indexing
- ✅ Binomial coefficient and probability
- ✅ Cumulative range sums with `math.comb()`
- ✅ Pythonic code with `sum()` and generator expressions
- ✅ f-string formatting
- ✅ Common syntax pitfalls

---

## 🧠 Reflection

- Writing and running actual code helped me internalize how Pascal’s Triangle connects to binomial probability and central distributions.  
- It was especially interesting to see that **math problems can be translated into code**, allowing me to verify abstract ideas in a concrete way.  
- I also found it fascinating that while code provides exact answers, **the human brain can still optimize logic or notice patterns** that guide the approach even before writing code.  
- Small syntax errors — like writing `=+` instead of `+=`, or forgetting to prefix strings with `f` — had a surprisingly big impact, and reminded me to write mindfully.  
- Overall, it was exciting to realize how **math, logic, and programming work hand-in-hand** to solve real problems — and how mastering both brings deeper insight and stronger problem-solving skills.
