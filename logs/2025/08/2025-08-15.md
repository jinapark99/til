# 2025-08-15 (FRI) | Eye-Tracking (Parkinson) – Code Refactor & Measurement Logic Change

## Today I Learned (TIL)
- **Use iris landmarks** as gaze centers to capture vertical (Y) movement sensitivity.
- **Coordinate system unification** (Frame → MON) is required for accurate comparison between stimulus and gaze points.
- **Baseline capture** ("snapshot") should only compute `baseline_io` without applying scale/adjustment to avoid flicker and errors.
- **External webcams** may open at reduced resolutions (e.g., 640×360) even if capable of 1080p, leading to lower vertical sensitivity → force 720p+ where possible.
- **Without additional sensors**, it was impossible to verify whether the unified coordinate tracking truly matched real eye movement — decided to shift the core logic to measure **reaction time latency** instead.
- Today's updated code did **not run successfully**; priority is to adapt logic for latency analysis in the next iteration.

---

## Problem Summary
- CSV/graphs show minimal vertical gaze variation — appears to track only horizontally.
- Perceived “screen off” during snapshot baseline.
- External cam resolution unexpectedly low → halved vertical sensitivity.
- No sensor available to verify coordinate alignment accuracy in real-time → accuracy measurement approach not viable.

---

## Concepts Practiced
- Mediapipe FaceMesh (`refine_landmarks=True`) **iris landmarks** (468–477)
- **Coordinate transformation** (Frame → MON scaling)
- **Distance scaling**: `baseline_io / current_io`
- Camera resolution negotiation (FOURCC/MJPG, AVFoundation)
- Shift from position error tracking to **reaction time latency** measurement logic

---

## Python Practice (Core Patch Snippets)

### 1) Iris center as gaze position
```python
RIGHT_IRIS = [468, 469, 470, 471, 472]
LEFT_IRIS  = [473, 474, 475, 476, 477]

def iris_center(lms, ids, w, h):
    xs, ys = zip(*[get_xy(lms, i, w, h) for i in ids])
    return sum(xs)/len(xs), sum(ys)/len(ys)

def compute_metrics(lms, w, h):
    # Eye corner distance for scaling
    rx, ry = get_xy(lms, R_OUT, w, h); rix, riy = get_xy(lms, R_IN, w, h)
    lx, ly = get_xy(lms, L_OUT, w, h); lix, liy = get_xy(lms, L_IN, w, h)
    io = ((lx - rx)**2 + (ly - ry)**2)**0.5

    # Gaze center = iris center
    r_cx, r_cy = iris_center(lms, RIGHT_IRIS, w, h)
    l_cx, l_cy = iris_center(lms, LEFT_IRIS,  w, h)

    # Face center from eye corners
    fcx, fcy = (rx + lx)/2, (ry + ly)/2

    return dict(
        inter_ocular_px=io,
        right_cx=r_cx, right_cy=r_cy,
        left_cx=l_cx,  left_cy=l_cy,

      face_cx=fcx,   face_cy=fcy
    )
```

---

### 2) Stabilize baseline snapshot
```python
io_vals = []
t_end = time.time() + 0.8
last_canvas = None
while time.time() < t_end:
    ret, frame = cap.read()
    if not ret: break
    frame = cv2.flip(frame, 1)

    canvas = cv2.resize(frame, (MON_W, MON_H))
    canvas = put_korean_center(canvas, "찰칵!", 48, (255, 0, 0))
    cv2.imshow(WIN, canvas); cv2.waitKey(1)
    last_canvas = canvas

    res = fm.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    if res.multi_face_landmarks:
        m = compute_metrics(res.multi_face_landmarks[0].landmark, w, h)
        if m["inter_ocular_px"] > 1e-6:
            io_vals.append(m["inter_ocular_px"])

baseline_io = float(np.median(io_vals)) if io_vals else 1.0
print(f"[baseline_io] {baseline_io:.2f}px")
```

---

### 3) Coordinate unification (Frame → MON)
```python
sx, sy = MON_W / w, MON_H / h

face_cx_mon = m["face_cx"] * sx; face_cy_mon = m["face_cy"] * sy
right_x_adj_mon = right_x_adj * sx; right_y_adj_mon = right_y_adj * sy
left_x_adj_mon  = left_x_adj  * sx; left_y_adj_mon  = left_y_adj  * sy

stim_x_centered = stim_x - face_cx_mon
stim_y_centered = stim_y - face_cy_mon
right_eye_x_adj_c = right_x_adj_mon - face_cx_mon
right_eye_y_adj_c = right_y_adj_mon - face_cy_mon
left_eye_x_adj_c  = left_x_adj_mon  - face_cx_mon
left_eye_y_adj_c  = left_y_adj_mon  - face_cy_mon
```

---

### Insight
- Feature point choice defines sensitivity: eye corners are stable but poor for vertical tracking; iris center captures vertical motion well.
- Coordinate mismatch can make tracking appear non-functional.
- Resolution is measurement fidelity: post-scaling can't recover lost detail — capture quality must be ensured at source.
- Without external ground truth sensors, coordinate match accuracy can’t be validated → reaction time latency is a more feasible metric.

---

### Reflection
- Today’s main limitation: no hardware to validate coordinate tracking accuracy.
- Logic shifted from position accuracy measurement to reaction time latency analysis, which can be computed from existing CSV data without extra sensors.
- Code changes couldn’t be fully tested due to runtime issues — priority is to fix execution and validate latency logic in the next iteration.

---

### Next Steps
- Implement latency measurement: time difference between stim position change and corresponding eye movement onset.
- Develop error metrics (dx, dy, RMSE) for simulation/testing environments with synthetic data.
- Add catch-up saccade detection for Parkinson’s-specific eye movement patterns.
- Ensure high-resolution capture (≥720p) on all external webcams.
- Test new logic end-to-end with actual data and verify latency outputs.
