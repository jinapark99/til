# 🗓️ 2025-08-01 (FRI)

> **Today I learned**  
What it truly means to understand how the internet communicates, from the foundational protocols to practical web scraping using Python.  
I didn't just learn to code — I learned to interpret, question, and reconstruct how machines talk to each other, and how I can guide them step by step.

---

## 🌐 Internet & Web Communication – Questions I Asked

Throughout today (and yesterday), I explored foundational web concepts not by memorizing, but by **relentlessly asking why**. Some of the major questions I asked and understood deeply:

### ❓ Who created all this? Where did these ideas come from?

- I asked: *“Who first imagined this whole system of invisible connections and layered protocols?”*
- I learned that the internet isn’t just a machine thing — it’s a human design:
  - **Paul Baran** (RAND): Invented packet switching (data chopped into pieces)
  - **Vint Cerf & Bob Kahn**: Designed **TCP/IP**, the heart of internet communication
  - **Tim Berners-Lee** (CERN): Invented **HTML** and the **World Wide Web** in 1989
- These weren’t just programmers — they were **architects of connection itself**.
- And now I use their legacy in 5 lines of Python.

---

### ❓ What is a socket? Why do we use `socket.socket()`?
- A socket is like a **virtual phone line** — it lets two computers talk.
- `socket.socket()` creates the socket; the first `socket` is the module, the second is the class.
- Analogy: `socket.socket()` = "Get me a telephone from the telephone toolbox."

### ❓ What do `AF_INET` and `SOCK_STREAM` mean?
- `AF_INET` → IPv4 address format (like house addresses)
- `SOCK_STREAM` → TCP protocol = safe delivery (like hand-delivering a message)
- These define how data flows — reliably (TCP) or just fast (UDP).

---

### ❓ What is a byte? Why do we encode and decode?

- A byte is a group of **8 bits**, the smallest building block of data (like a Lego brick).
- But the **meaning** of that byte depends on the encoding standard used.

#### 🔤 Common Byte Encodings:

| Encoding | Description |
|----------|-------------|
| **ASCII** | 7-bit English-only standard (e.g., 'A' = 65) |
| **UTF-8** | Variable-length (1–4 bytes), can represent all characters in the world |
| **UTF-16** | 2 bytes for most characters, sometimes 4 |
| **UTF-32** | Fixed 4 bytes per character (simple, but wasteful) |

✅ **Today, UTF-8 is the dominant standard.**  
It’s used by over 95% of websites and is Python’s default in most cases.

#### 🧠 How Python handles it:
- `encode()` = string (Unicode) → bytes
- `decode()` = bytes → string (Unicode)
- `ord()` & `chr()` help map characters to their numeric codes (Unicode points)

---

### ❓ Why are SSL errors ignored sometimes?

- Websites use certificates (SSL) to prove they’re secure.
- But for scraping, expired/mismatched certs block access.

```python
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE
```

---

### ❓ What is HTML parsing and why is it hard?
- Webpages look clean in a browser but are often messy in HTML.
- Tags are unclosed, deeply nested, or dynamically generated.
- I asked why BeautifulSoup exists → to fix and structure messy HTML.

### 🧠 So what is a parser?
A parser is a translator that turns raw HTML text into structured elements (tree-like).
It lets Python say things like:

- "Get me all the <a> tags"
- "Pull the value of the href attribute"
- "Grab the contents inside a tag"

---

### 🧪 BeautifulSoup + urllib – Practical Learning
After building intuition, I used BeautifulSoup and urllib to:

- Retrieve a webpage
- Parse all <a> (anchor) tags
- Extract the href attribute
- Traverse the Nth link, M times
- Print the final name on the last page

## ✅ Final Code: Follow Nth Link M Times
```python
import urllib.request
from bs4 import BeautifulSoup
import ssl

ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE

url = input('Enter URL: ')
count = int(input('Enter count: '))
position = int(input('Enter position: '))

for i in range(count):
    html = urllib.request.urlopen(url, context=ctx).read()
    soup = BeautifulSoup(html, 'html.parser')
    tags = soup('a')
    url = tags[position - 1].get('href')

html = urllib.request.urlopen(url, context=ctx).read()
soup = BeautifulSoup(html, 'html.parser')
print('Last name:', soup.title.string)
````

---

### 💡 Insight – What I Realized
I don’t need to memorize syntax. What matters is knowing the structure and flow.
I’m not afraid to ask small, foundational questions like:

- “Why does decode need utf-8?”
- “Why is data a byte, not a string?”
- The internet is layered: from hardware → protocol → string → meaning.
- Even a simple line like tag.contents[0] hides a rich structure beneath it.
- I also learned that websites often block scrapers with JS, dynamic HTML, or timing tricks — and that BeautifulSoup or Selenium can overcome them.

---

### 🔗 Topics Covered
Who invented the internet and HTML? (Paul Baran, Vint Cerf, Tim Berners-Lee)

- Sockets: what they are, how to use them
- IP, ports, and TCP/UDP communication
- Byte structure, encoding/decoding, Unicode, UTF-8
- HTML structure and parser basics
- urllib for HTTP requests
- BeautifulSoup for HTML parsing
- Navigating <a> links repeatedly
- Extracting attributes cleanly


